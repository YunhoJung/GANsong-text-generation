{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "real crawler.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "LGM5IpzO_gqC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import lxml.html"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vj_eRNte_gqH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "extract_song_number = re.compile(\"[0-9]{1,}\")\n",
    "\n",
    "def crawler():\n",
    "    artist_number = int(input(\"아티스트 번호를 입력하세요.\"))\n",
    "    song_page = int(input(\"아티스트의 노래 페이지 수를 입력하세요.\"))\n",
    "    d = webdriver.Chrome(\"D:\\chromedriver.exe\")\n",
    "    \n",
    "    total_title_list = []\n",
    "    total_song_number_list = []\n",
    "    lyrics_list = []\n",
    "    \n",
    "    for page_nubmer in range(1, song_page+1):\n",
    "        d.implicitly_wait(3)\n",
    "        url_s = 'https://music.naver.com/artist/track.nhn?artistId='+str(artist_number)+'&page=' + str(page_nubmer)\n",
    "        d.get(url_s)\n",
    "        \n",
    "        html = d.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        title = soup.select('.track')\n",
    "        temp_title_list = [i.text.strip() for i in title]\n",
    "        temp_song_number_list = [extract_song_number.findall(i.get('onclick'))[0] for i in title]\n",
    "        \n",
    "        total_title_list += temp_title_list\n",
    "        total_song_number_list += temp_song_number_list\n",
    "        \n",
    "    for song_number in total_song_number_list:\n",
    "        url2 = 'https://music.naver.com/lyric/index.nhn?trackId=' + str(song_number)\n",
    "        response = requests.get(url2)\n",
    "        html = response.text \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        lyrics = soup.select('.section_lyrics')\n",
    "        temp_lyrics = str(lyrics[0]).replace(\"\"\"<div class=\"section_lyrics\">\\n<div class=\"show_lyrics\" id=\"lyricText\">\"\"\", \"\").replace(\"</div>\\n</div>\", \"\")\n",
    "        temp_lyrics2 = temp_lyrics.strip().replace(\"<br/>\", \"\\n\")\n",
    "        temp_lyrics3 = re.sub('\\n\\n\\n+','\\n',temp_lyrics2)\n",
    "        lyrics_list.append(temp_lyrics3)\n",
    "    \n",
    "    df = pd.DataFrame({\"artist_number\" : [artist_number for _ in range(len(total_title_list))],\n",
    "                 \"title\" : total_title_list, \"lyrics\" : lyrics_list})\n",
    "    \n",
    "    df.drop_duplicates(\"title\", inplace = True)\n",
    "    df.drop_duplicates(\"lyrics\", inplace = True)\n",
    "    \n",
    "    df.drop(index = df[df.lyrics.apply(lambda x : 1 if \"\"\"<div class=\"section_lyrics\">\"\"\" in x else 0) == 1].index, inplace = True)\n",
    "    df.index = np.arange(len(df))\n",
    "    \n",
    "    return df"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T1ifHk4Q_gqJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "df = crawler()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oquub_pe_gqO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "df.to_csv(\"김광석.csv\", index = False, encoding = 'utf-8-sig')"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}